{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f47fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import (Reshape,LeakyReLU,Dropout,Conv2DTranspose, Add, Conv2D, MaxPool2D, Dense,\n",
    "                                     Flatten, InputLayer, BatchNormalization, Input, )\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d jessicali9530/celeba-dataset\n",
    "!unzip \"/content/celeba-dataset.zip\" -d \"/content/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd55f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IM_SHAPE = (64,64,3)\n",
    "LEARNING_RATE = 2e-4\n",
    "LATENT_DIM=100\n",
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/content/dataset/img_align_celeba/img_align_celeba\", label_mode=None, image_size=(IM_SHAPE[0], IM_SHAPE[1]), batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04edfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "  return tf.cast(image, tf.float32) / 127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c337039",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    dataset\n",
    "    .map(preprocess)\n",
    "    .unbatch()\n",
    "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
    "    .batch(BATCH_SIZE,drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7068f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataset.take(1):\n",
    "  print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8836db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "k=0\n",
    "n = 4\n",
    "for i in range(n):\n",
    "  ax = plt.subplot(2,2, k+1)\n",
    "  plt.imshow((d[i]+1)/2)\n",
    "  plt.axis(\"off\")\n",
    "  k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=tf.keras.Sequential([\n",
    "  Input(shape=(LATENT_DIM,)),\n",
    "  Dense(4*4*LATENT_DIM),\n",
    "  Reshape((4,4,LATENT_DIM)),\n",
    "\n",
    "  Conv2DTranspose(512,kernel_size=4,strides=2, padding='same'),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(0.2),\n",
    "\n",
    "  Conv2DTranspose(256,kernel_size=4,strides=2, padding='same'),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(0.2),\n",
    "\n",
    "  Conv2DTranspose(128,kernel_size=4,strides=2, padding='same'),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(0.2),\n",
    "\n",
    "  Conv2DTranspose(3,kernel_size=4,strides=2, activation=tf.keras.activations.tanh, padding='same'),\n",
    "\n",
    "],name='generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e600bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45468869",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator=tf.keras.Sequential([\n",
    "  Input(shape=(IM_SHAPE[0],IM_SHAPE[1],3)),\n",
    "\n",
    "  Conv2D(64,kernel_size=4,strides=2, padding='same'),\n",
    "  LeakyReLU(0.2),\n",
    "\n",
    "  Conv2D(128,kernel_size=4,strides=2, padding='same'),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(0.2),\n",
    "  \n",
    "  Conv2D(256,kernel_size=4,strides=2, padding='same'),\n",
    "  BatchNormalization(),\n",
    "  LeakyReLU(0.2),\n",
    "\n",
    "  Conv2D(1,kernel_size=4,strides=2, padding='same'),\n",
    "\n",
    "  Flatten(),\n",
    "  Dense(1,activation='sigmoid')\n",
    "  \n",
    "\n",
    "],name='discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f92f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowImage(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        n=6\n",
    "        k=0\n",
    "        out=self.model.generator(tf.random.normal(shape=(36, self.latent_dim)))\n",
    "        plt.figure(figsize=(16,16))\n",
    "        for i in range(n):\n",
    "          for j in range(n):\n",
    "            ax=plt.subplot(n,n,k+1)\n",
    "            plt.imshow((out[k]+1)/2,)\n",
    "            plt.axis('off')\n",
    "            k+=1\n",
    "        plt.savefig(\"generated/gen_images_epoch_{}.png\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "  def __init__(self,discriminator,generator):\n",
    "    super(GAN,self).__init__()\n",
    "    self.discriminator=discriminator\n",
    "    self.generator=generator\n",
    "\n",
    "  def compile(self,d_optimizer,g_optimizer,loss_fn):\n",
    "    super(GAN,self).compile()\n",
    "    self.d_optimizer=d_optimizer\n",
    "    self.g_optimizer=g_optimizer\n",
    "    self.loss_fn=loss_fn\n",
    "    self.d_loss_metric=tf.keras.metrics.Mean(name='d_loss')\n",
    "    self.g_loss_metric=tf.keras.metrics.Mean(name='g_loss')\n",
    "    \n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [self.d_loss_metric,self.g_loss_metric]\n",
    "  \n",
    "  def train_step(self,real_images):\n",
    "    batch_size=tf.shape(real_images)[0]\n",
    "\n",
    "    ######## Discriminator\n",
    "    random_noise=tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
    "    fake_images=self.generator(random_noise)\n",
    "\n",
    "    real_labels=tf.ones((batch_size,1))+0.25*tf.random.uniform((batch_size,1),minval=-1,maxval=1)\n",
    "    fake_labels=tf.zeros((batch_size,1))+0.25*tf.random.uniform((batch_size,1),)\n",
    "\n",
    "    with tf.GradientTape() as recorder:\n",
    "      real_predictions=self.discriminator(real_images)\n",
    "      d_loss_real=self.loss_fn(real_labels,real_predictions)\n",
    "\n",
    "      fake_predictions=self.discriminator(fake_images)\n",
    "      d_loss_fake=self.loss_fn(fake_labels,fake_predictions)\n",
    "\n",
    "      d_loss=d_loss_real+d_loss_fake\n",
    "      \n",
    "    partial_derivatives = recorder.gradient(d_loss,self.discriminator.trainable_weights)\n",
    "    self.d_optimizer.apply_gradients(zip(partial_derivatives, self.discriminator.trainable_weights))\n",
    "\n",
    "    ############# Generator\n",
    "    random_noise=tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
    "    flipped_fake_labels=tf.ones((batch_size,1))\n",
    "\n",
    "    with tf.GradientTape() as recorder:\n",
    "\n",
    "      fake_predictions=self.discriminator(self.generator(random_noise))\n",
    "      g_loss=self.loss_fn(flipped_fake_labels,fake_predictions)\n",
    "      \n",
    "    partial_derivatives = recorder.gradient(g_loss,self.generator.trainable_weights)\n",
    "    self.g_optimizer.apply_gradients(zip(partial_derivatives, self.generator.trainable_weights))\n",
    "\n",
    "    self.d_loss_metric.update_state(d_loss)\n",
    "    self.g_loss_metric.update_state(g_loss)\n",
    "    \n",
    "    return {'d_loss':self.d_loss_metric.result(),\n",
    "            'g_loss':self.g_loss_metric.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fec159",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan=GAN(discriminator,generator)\n",
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,beta_1=0.5),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,beta_1=0.5),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1000\n",
    "history=gan.fit(train_dataset,epochs=EPOCHS,callbacks=[ShowImage(LATENT_DIM)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['d_loss'])\n",
    "plt.plot(history.history['g_loss'])\n",
    "plt.title('GAN Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['d_loss', 'g_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
